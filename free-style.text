I'm going to speak with your today about the verifibility crisis in
science tech writing and life. That is a long title; it barely fits on
the slide...I'm going to suggest that in recent years, we have
experienced issues in each of these domains and that these issues have
commonality that makes them yield to similar solutions.

From a technical writing standpoint, this talk is addressed mostly
toward writers who write for a developer audience. However, the impact
of what I discuss on our social experience should be relevant to
everyone here.

Our story begins in XXXX with The Experiement Experiment. This was a
project initiated by XXXX in which they identified 100 published
peer-reviewed research papers. For each paper, a team of investigators
studied the methodology described in the paper and then attempted to
recreate the results that the authors of the paper obtained.

for example, ....

what investigators found is that in 71 out of 100 papers, the results
obtained by the original researchers could not be duplicated. That's
71%.

Now, before we jump to conclusions, we don't know precisely why the
results of these studies couldn't be duplicated. It could be that the
particular conditions of the original study couldn't be recreated, or
maybe the paper didn't provide enough information for how to duplicated
the study, or perhaps the paper misrepresented the results of the study.
We really don't know, but irrespective of the reasons, the inability to
recreate--or verify--the published results of scientific research
potentially creates a lack of confidence.

At this point, I want to introduce a word: veracity. One of the great
things about speaking at a technical writing conference is that I don't
generally have to be concerned that my audience won't understand a given
word. That said, here is the definition.

DEFN: Veracity

So veracity means can some person--or information--be trusted in its
correctness, accuracy, truth.

I'm proposing that verifiability ensures veracity. 

[ logic diagram with implies ]

a fine point here, verifiability doesn't mean that you have to do the
work of verifying the information, it means that you _could_. 

Back to our story...

After The Experiment Experiment, other investigators conducted similar
studies with simiar results and all of this led to what became know as
the Reproducibility Crisis in Science.

Research scientists feeling assailed by all of this were looking for a
way to respond.

So in response, Roger Peng, a professor in the department of
biostatistics at Johns-Hopkins developed a set of guidelines for
Reproducible Research.

Reproducible research doesn't require that a study be recreatable. Peng
felt that, as a standard, that was too high a bar. Instead Reproducible
Research says that a published paper must also make available the raw
data that was collected and the code that was used to analyze the data.

Enter: RMarkdown

This guy Yihui blah blah saw that an elegant way to support reproducible
research would be to extend the Markdown language to enable R
programming code to be embedded in Markdown documents.

What is Markdown?

Markdown is a light-weight markup language that has received wide-spread
adoption in recent years. It was invented by John Gruber of Daring
Fireball. 

Provide links to gruber and DF twitter feeds.

Here is a Markdown document. 

Here is what that looks like when published.


What is R?

R is an open-source programming language that is designed specifically
for statistical analysis.

Here is some R program code that calculates the Fibonacci sequence.

And here is some R code embedded in a Markdown document. 

And here is what that looks like:

So by default, the output from R is included in the document text.

When you build the RMarkdown document, the embedded code is executed and
at your option, the output is included in the published document.

The implications of this for researchers is that the electronic version
of the research paper doesn't just display the code that was used to 
analyze the data, building the document actually recreates the analysis
of the data.

RMarkdown Output

RMarkdown leverages Pandoc to process the markdown component of the
source document. So RMarkdown supports any output format supported by
Pandoc.

And I have kind of a funny story about that . . . at HBO, the technical
writing team is part of the larger Design organization and I report to
the VP of design, Ryan. Well, Ryan is a Keynote afficianado and was
somewhat aghast at my slides . . . I am not a designer.

Reproducible Research and RMarkdown has taken hold in the Data Science
and Statistics communities, but it hasn't been all Rainbows and
Unicorns. In <MONTH>, the New England Journal of Medicine published an
op-ed that said that they were not getting onboard with Reproducible
Research. Their view is that the data that comes out of scientific
research is the IP of the researchers and papers that our published in
the New England Journal will not be accompanied by data. 

So we will have to see how this all unfolds over the next few years.

Technical Writing

When Yuhui, designed knitr, he foresaw that researchers might want to
use programming languages other than R for their analysis. So it made
the language processor extensible . . . and then proceeded to extend it.

Here is a list of the languages currently supported in RMarkdown. So,
here is fibonnaci written in Python.




---

I'm sure that many of you are aware that Hillary Clinton won a majority
of the popular vote. And obviously, President Trump won a majority of
the eletoral votes.

But there hasn't been a lot of discussion about how the popular vote
margin, affected the electoral vote margin.

Well, Nate Silver published an article titled 

What a difference two percentage points makes
https://fivethirtyeight.com/features/what-a-difference-2-percentage-points-makes/

So let's look at an intelligent document that verifies that what Nate
says is true.


